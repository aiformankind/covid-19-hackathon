{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleDiseaseOutbreakPrediction.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu5YfpDZ6eCDEhBlCG4HuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaniska/covid-19-hackathon/blob/analyze-streaming-data/SampleDiseaseOutbreakPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6KBCWoelqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Explore Test Data and Run Saved Logistics Regression Model to predict outbreak of flu\n",
        "\n",
        "import org.apache.spark.sql.Dataset\n",
        "import org.apache.spark.sql.Encoders\n",
        "import org.apache.spark.sql.Row\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.streaming.StreamingQuery\n",
        "import org.apache.spark.sql.streaming.StreamingQueryException\n",
        "import org.apache.spark.sql.types.StructType\n",
        "\n",
        "val tweetSchema = new StructType()\n",
        "                .add(\"tweetId\", \"string\")\n",
        "                .add(\"tweetText\", \"string\")\n",
        "                .add(\"location\", \"string\")\n",
        "                .add(\"timestamp\", \"string\");\n",
        "\n",
        "val spark = SparkSession\n",
        "\t\t.builder()\n",
        "\t\t.appName(\"StreamHandler\")\n",
        "\t\t.config(\"spark.master\", \"local\")\n",
        "\t\t.getOrCreate();\n",
        "            \n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.functions.{unix_timestamp, to_date}\n",
        "\n",
        "val realTweets = \"/home/opt/data/test2/\"\n",
        "\n",
        "val tweetStream = spark.readStream\n",
        "\t\t       .schema(tweetSchema)\n",
        "\t\t       .option(\"maxFilesPerTrigger\", 1)\n",
        "\t\t       .json(realTweets)\n",
        "\t\t       .select($\"location\", to_date(unix_timestamp($\"timestamp\", \"EEE MMM dd HH:mm:ss Z yyyy\").cast(\"timestamp\")).as(\"timestamp\"))\n",
        " \n",
        " val streamingCountsDF = \n",
        "  tweetStream\n",
        "    .groupBy($\"location\", $\"timestamp\" , window($\"timestamp\", \"1 hour\"))\n",
        "    .count()\n",
        "    \n",
        "streamingCountsDF.isStreaming\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\") \n",
        "\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.streaming._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val query =\n",
        "  streamingCountsDF\n",
        "    .writeStream\n",
        "    .format(\"memory\")     \n",
        "    .trigger(ProcessingTime(\"10 seconds\"))\n",
        "    .queryName(\"testTable2\")     \n",
        "    .outputMode(\"complete\") \n",
        "    .start()\n",
        "\n",
        "%sql\n",
        "select location, weekofyear(window.end) as Week, AVG(count) \n",
        "as weekly_avg from testTable2 where count > 0 group by location,  weekofyear(window.end) \n",
        "having count(*) > 0 order by weekly_avg DESC\n",
        "\n",
        "//California\t3\t7.666666666666667\n",
        "//California\t1\t7.0\n",
        "//Texas\t5\t6.0\n",
        "//Texas\t4\t4.857142857142857\n",
        "//California\t4\t4.714285714285714\n",
        "\n",
        "//\n",
        "\n",
        "%sql\n",
        "select location, date_format(window.end, \"dd-MM-YYYY\") as time, AVG(count) \n",
        "as count from testTable2 where count > 0 group by location,  date_format(window.end, \"dd-MM-YYYY\") \n",
        "having count(*) > 0 order by count DESC\n",
        "\n",
        "//\n",
        "//California\t04-01-2017\t14.0\n",
        "//California\t21-01-2017\t11.0\n",
        "//California\t17-01-2017\t11.0\n",
        "//California\t13-01-2017\t10.0\n",
        "//Florida\t09-01-2017\t9.0\n",
        "//Ohio\t18-01-2017\t9.0\n",
        "\n",
        "%sql\n",
        "select weekofyear(window.end) as Week from testTable2 where count > 0 group by  weekofyear(timestamp)\n",
        "\n",
        "// create the features for test data and run the prediction\n",
        "import org.apache.spark.sql.functions.udf\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.linalg.Vectors\n",
        "import org.apache.spark.ml.linalg._\n",
        "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.feature.StringIndexer\n",
        "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
        "import org.apache.spark.mllib.tree.RandomForest\n",
        "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
        "import org.apache.spark.mllib.util.MLUtils\n",
        "import org.apache.spark.mllib.regression.LabeledPoint\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
        "\n",
        "////\n",
        "val locationRankDF = sql(\"select row_number() over (order by location) as locRank, location as locName from testTable2  GROUP BY location\")\n",
        "locationRankDF.createOrReplaceTempView(\"locationRankMap\")\n",
        "println(\"Location Rank\")\n",
        "\n",
        "//\n",
        "def calcLabel: (Double => Double) = (arg: Double) => {if (arg > 2.5) 1.0 else 0.0 }\n",
        "\n",
        "// TESTING DATA SET\n",
        "\n",
        "val testDf1 = sql(s\"\"\"\n",
        "SELECT locRank, avg(count) as weeklyAvg, weekofyear(window.end) as week\n",
        "  FROM testTable2\n",
        "  JOIN locationRankMap ON location = locName\n",
        "  GROUP BY weekofyear(window.end), locRank\n",
        "  HAVING weeklyAvg > 0\n",
        "  order by weeklyAvg DESC\n",
        "  \"\"\")\n",
        "  \n",
        "//df1.show\n",
        "\n",
        "val testDf2 = testDf1.select($\"locRank\".cast(\"Double\"), $\"weeklyAvg\".cast(\"Double\"), $\"week\".cast(\"Double\"))\n",
        "val flulabel = udf(calcLabel)\n",
        "\n",
        "val testDf3 = testDf2.withColumn(\"class\", flulabel(testDf2(\"weeklyAvg\")))\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"locRank\", \"weeklyAvg\",\"week\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "val testDf4 = assembler.transform(testDf3)\n",
        "\n",
        "val labelIndexer = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")\n",
        "val testDf5 = labelIndexer.fit(testDf4).transform(testDf4)\n",
        "\n",
        "val splitSeed = 5043\n",
        "val Array(holdoutData2, testData) = testDf5.randomSplit(Array(0.0, 1.0), splitSeed)\n",
        "\n",
        "testData.show\n",
        "\n",
        "//////////////////////////////// ///////////////////////////////////// ////////////////////////////////////////\n",
        "\n",
        "val model = PipelineModel.load(\"/home/opt/models/lr-model4\")\n",
        "\n",
        "//println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
        "\n",
        "val predictions = model.transform(testData)\n",
        "\n",
        "print(\"$$$$$$ predictions size >>> \"+predictions.count())\n",
        "\n",
        "predictions.show\n",
        "\n",
        "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n",
        "val accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "val lp = predictions.select( \"label\", \"prediction\")\n",
        "val counttotal = predictions.count()\n",
        "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
        "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
        "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
        "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
        "val ratioCorrect=correct.toDouble/counttotal.toDouble\n",
        "\n",
        "val  predictionAndLabels =predictions.select(\"rawPrediction\", \"label\").rdd.map(x => (x(0).asInstanceOf[DenseVector](1), x(1).asInstanceOf[Double]))\n",
        "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
        "println(\"area under the precision-recall curve: \" + metrics.areaUnderPR)\n",
        "println(\"area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n",
        "\n",
        "//testData\n",
        "//+-------+------------------+----+-----+--------------------+-----+\n",
        "//|locRank|         weeklyAvg|week|class|            features|label|\n",
        "//+-------+------------------+----+-----+--------------------+-----+\n",
        "//|    1.0|               1.0| 2.0|  0.0|       [1.0,1.0,2.0]|  0.0|\n",
        "//|    1.0|               1.0|52.0|  0.0|      [1.0,1.0,52.0]|  0.0|\n",
        "//|    4.0|               1.0| 5.0|  0.0|       [4.0,1.0,5.0]|  0.0|\n",
        "//|    5.0|               4.0| 5.0|  1.0|       [5.0,4.0,5.0]|  1.0|\n",
        "//|    5.0|               4.0|52.0|  1.0|      [5.0,4.0,52.0]|  1.0|\n",
        "//|    5.0| 4.714285714285714| 2.0|  1.0|[5.0,4.7142857142...|  1.0|\n",
        "//|    5.0| 4.714285714285714| 4.0|  1.0|[5.0,4.7142857142...|  1.0|\n",
        "\n",
        "//predictions\n",
        "//|locRank|         weeklyAvg|week|class|            features|label|       rawPrediction|         probability|prediction|\n",
        "//+-------+------------------+----+-----+--------------------+-----+--------------------+--------------------+----------+\n",
        "//|    1.0|               1.0| 2.0|  0.0|       [1.0,1.0,2.0]|  0.0|[2.31958412810473...|[0.91048605239314...|       0.0|\n",
        "//|    1.0|               1.0|52.0|  0.0|      [1.0,1.0,52.0]|  0.0|[8.16989072889752...|[0.99971703115020...|       0.0|\n",
        "//|    1.0|              1.75| 4.0|  0.0|      [1.0,1.75,4.0]|  0.0|[0.96174491143835...|[0.72347102941338...|       0.0|\n",
        "//|    1.0|               2.0| 1.0|  0.0|       [1.0,2.0,1.0]|  0.0|[0.08010935515809...|[0.52001663518996...|       0.0|\n",
        "//|    1.0|2.3333333333333335| 3.0|  0.0|[1.0,2.3333333333...|  0.0|[-0.3933679277871...|[0.40290680424173...|       1.0|\n",
        "//|    4.0|               1.0| 5.0|  0.0|       [4.0,1.0,5.0]|  0.0|[2.79505199713013...|[0.94240785695609...|       0.0|\n",
        "//|    5.0|               4.0| 5.0|  1.0|       [5.0,4.0,5.0]|  1.0|[-3.5308707680029...|[0.02844651212230...|       1.0|\n",
        "//|    5.0|               4.0|52.0|  1.0|      [5.0,4.0,52.0]|  1.0|[1.96841743674228...|[0.87744102869336...|       0.0|\n",
        "//|    5.0| 4.714285714285714| 2.0|  1.0|[5.0,4.7142857142...|  1.0|[-5.3979381932867...|[0.00450551135944...|       1.0|\n",
        "//|    5.0| 4.714285714285714| 4.0|  1.0|[5.0,4.7142857142...|  1.0|[-5.1639259292550...|[0.00568667910063...|       1.0|\n",
        "//|    5.0|               7.0| 1.0|  1.0|       [5.0,7.0,1.0]|  1.0|[-10.366301218858...|[3.14745053482784...|       1.0|\n",
        "//+-------+------------------+----+-----+--------------------+-----+--------------------+--------------------+----------+\n",
        "\n",
        "//accuracy: Double = 0.941260162601624\n",
        "//lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
        "//counttotal: Long = 229\n",
        "//correct: Long = 220\n",
        "//wrong: Long = 9\n",
        "//truep: Long = 203\n",
        "//falseN: Long = 7\n",
        "//falseP: Long = 2\n",
        "//ratioWrong: Double = 0.039301310043668124\n",
        "//ratioCorrect: Double = 0.9606986899563319\n",
        "\n",
        "//area under the precision-recall curve: 0.8665970252411103\n",
        "//area under the (ROC) curve : 0.941260162601624"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}