{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleDiseaseOutbreakTraining.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP349iCWeHP5L4PwnmMwd38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaniska/covid-19-hackathon/blob/analyze-streaming-data/SampleDiseaseOutbreakTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GgJFHxwld9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// Training Model\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Encoders;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.streaming.StreamingQuery;\n",
        "import org.apache.spark.sql.streaming.StreamingQueryException;\n",
        "import org.apache.spark.sql.types.StructType\n",
        "\n",
        "val tweetSchema = new StructType()\n",
        "                .add(\"tweetId\", \"string\")\n",
        "                .add(\"tweetText\", \"string\")\n",
        "                .add(\"location\", \"string\")\n",
        "                .add(\"timestamp\", \"string\");\n",
        "\n",
        "val spark = SparkSession\n",
        "\t\t.builder()\n",
        "\t\t.appName(\"StreamHandler\")\n",
        "\t\t.config(\"spark.master\", \"local\")\n",
        "\t\t.getOrCreate();\n",
        "\n",
        "val traingDataSet = \"/home/opt/data/training/\"\n",
        "\n",
        "val trainingDS = spark.read.json(traingDataSet).select($\"location\", to_date(unix_timestamp($\"timestamp\", \"EEE MMM dd HH:mm:ss Z yyyy\").cast(\"timestamp\")).as(\"timestamp\"))\n",
        "\n",
        "val trainingCountsDF = trainingDS\n",
        "    .groupBy($\"location\", $\"timestamp\")\n",
        "    .count()\n",
        "    \n",
        "trainingCountsDF.createOrReplaceTempView(\"trainingTable\")\n",
        "\n",
        "trainingCountsDF.show\n",
        "// shows counts per location per day \n",
        "// |   California|2016-02-08|    2|\n",
        "// |     New York|2016-02-08|    2|\n",
        "// |   California|2016-02-07|    2|\n",
        "\n",
        "\n",
        "%sql\n",
        "select location, weekofyear(timestamp) as Week, AVG(count) \n",
        "as weekly_avg from trainingTable where count > 0 group by  weekofyear(timestamp) , location\n",
        "having count(*) > 0 order by weekly_avg DESC\n",
        "\n",
        "//location▼  Week▼        weekly_avg▼\n",
        "//Georgia\t      16\t4.5\n",
        "//California\t7\t4.428571428571429\n",
        "//Texas\t       16\t4.2\n",
        "//California\t17\t4.0\n",
        "//South Carolina\t8\t4.0\n",
        "//South Carolina\t16\t4.0\n",
        "//New York\t11\t3.75\n",
        "\n",
        "%sql\n",
        "select location, date_format(timestamp, \"dd-MM-YYYY\") as time, AVG(count) \n",
        "as count from trainingTable where count > 0 group by date_format(timestamp, \"dd-MM-YYYY\") , location\n",
        "having count(*) > 0 order by time\n",
        "\n",
        "%sql\n",
        "select weekofyear(timestamp) as Week from trainingTable where count > 0 group by  weekofyear(timestamp)\n",
        "\n",
        "/// create the features\n",
        "import org.apache.spark.sql.functions.udf\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.linalg.Vectors\n",
        "import org.apache.spark.ml.linalg._\n",
        "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.feature.StringIndexer\n",
        "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
        "import org.apache.spark.mllib.tree.RandomForest\n",
        "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
        "import org.apache.spark.mllib.util.MLUtils\n",
        "import org.apache.spark.mllib.regression.LabeledPoint\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
        "\n",
        "val locationRankDF = sql(\"select row_number() over (order by location) as locRank, location as locName from trainingTable  GROUP BY location\")\n",
        "locationRankDF.createOrReplaceTempView(\"locationRankMap\")\n",
        "println(\"Location Rank\")\n",
        "\n",
        "////\n",
        "def calcLabel: (Double => Double) = (arg: Double) => {if (arg > 2.5) 1.0 else 0.0 }\n",
        "\n",
        "// TRAINING DATA SET\n",
        "\n",
        "val trngDf1 = sql(s\"\"\"\n",
        "SELECT locRank, avg(count) as weeklyAvg, weekofyear(timestamp) as week\n",
        "  FROM trainingTable\n",
        "  JOIN locationRankMap ON location = locName\n",
        "  GROUP BY weekofyear(timestamp), locRank\n",
        "  HAVING weeklyAvg > 0\n",
        "  order by weeklyAvg DESC\n",
        "  \"\"\")\n",
        "  \n",
        "val trngDf2 = trngDf1.select($\"locRank\".cast(\"Double\"), $\"weeklyAvg\".cast(\"Double\"), $\"week\".cast(\"Double\"))\n",
        "\n",
        "println(\"Training DF 2\")\n",
        "trngDf2.show\n",
        "\n",
        "val flulabel = udf(calcLabel)\n",
        "\n",
        "val trngDf3 = trngDf2.withColumn(\"class\", flulabel(trngDf2(\"weeklyAvg\")))\n",
        "\n",
        "println(\"Training DF 3\")\n",
        "trngDf3.show\n",
        "\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"locRank\", \"weeklyAvg\",\"week\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "val trngDf4 = assembler.transform(trngDf3)\n",
        "\n",
        "val labelIndexer = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")\n",
        "val trngDf5 = labelIndexer.fit(trngDf4).transform(trngDf4)\n",
        "\n",
        "val splitSeed = 5043\n",
        "val Array(trainingData, validationData) = trngDf5.randomSplit(Array(0.7, 0.3), splitSeed)\n",
        "\n",
        "trainingData.show\n",
        "\n",
        "// Label distribution\n",
        "trainingData.groupBy(\"class\").count().show()\n",
        "trainingData.groupBy(\"class\").agg(max(\"weeklyAvg\").alias(\"max\")).show()\n",
        "//+-----+-----+\n",
        "//|class|count|\n",
        "//+-----+-----+\n",
        "//|  0.0|  309|\n",
        "//|  1.0|   23|\n",
        "//+-----+-----+\n",
        "\n",
        "//+-----+---+\n",
        "//|class|max|\n",
        "//+-----+---+\n",
        "//|  0.0|2.5|\n",
        "//|  1.0|6.5|\n",
        "+-----+---+\n",
        "\n",
        "//////////////////////////////// ///////////////////////////////////// ////////////////////////////////////////\n",
        "\n",
        "// CTREATE THE MODEL\n",
        "// val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
        "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.0234).setTol(0.00000001).setElasticNetParam(0.8)\n",
        "\n",
        "val pipeline = new Pipeline()\n",
        "  .setStages(Array(lr))\n",
        "  \n",
        "val model = pipeline.fit(trainingData)  \n",
        "import org.apache.spark.sql.functions.udf\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.linalg.Vectors\n",
        "import org.apache.spark.ml.linalg._\n",
        "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.feature.StringIndexer\n",
        "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
        "import org.apache.spark.mllib.tree.RandomForest\n",
        "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
        "import org.apache.spark.mllib.util.MLUtils\n",
        "import org.apache.spark.mllib.regression.LabeledPoint\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
        "//////////////////////////////////////////////////////////////// ////////////////////////////////////////////////////////////////\n",
        "\n",
        "val locationRankDF = sql(\"select row_number() over (order by location) as locRank, location as locName from trainingTable  GROUP BY location\")\n",
        "\n",
        "locationRankDF.createOrReplaceTempView(\"locationRankMap\")\n",
        "\n",
        "println(\"Location Rank\")\n",
        "\n",
        "//////////////////////////////////////////////////////////////// ////////////////////////////////////////////////////////////////\n",
        "\n",
        "def calcPercentLabel: (Double => Double) = (arg: Double) => {if (arg > 1.1) 1.0 else 0.0 }\n",
        "\n",
        "// TRAINING DATA SET\n",
        "\n",
        "val trngDf1 = sql(s\"\"\"\n",
        "SELECT locRank, sum(count) as weeklySum, weekofyear(timestamp) as week\n",
        "  FROM trainingTable\n",
        "  JOIN locationRankMap ON location = locName\n",
        "  GROUP BY weekofyear(timestamp), locRank\n",
        "  HAVING weeklySum > 0\n",
        "  order by weeklySum DESC\n",
        "  \"\"\")\n",
        "  \n",
        "val trngDf2 = trngDf1.select($\"locRank\".cast(\"Double\"), $\"weeklySum\".cast(\"Double\"), $\"week\".cast(\"Double\")).toDF(\"locRank\", \"weekSum\", \"week\")\n",
        "\n",
        "def zeroToOne: (Double => Double) = (arg: Double) => {if (arg==0) 1.0 else arg}\n",
        "\n",
        "val trngDfPrevWeek = trngDf1.select($\"locRank\".cast(\"Double\"), $\"weeklySum\".cast(\"Double\"), $\"week\".cast(\"Double\")+1).toDF(\"locRank\", \"weekSumPrev\", \"week\")\n",
        "\n",
        "val weekOverWeek = trngDf2.join(trngDfPrevWeek, Seq(\"locRank\", \"week\")).select($\"locRank\", $\"week\", $\"weekSum\"/$\"weekSumPrev\").toDF(\"locRank\", \"week\", \"weekOverWeek\")\n",
        "\n",
        "println(\"Week over week df\")\n",
        "weekOverWeek.show\n",
        "\n",
        "println(\"Training DF 2\")\n",
        "trngDf2.show\n",
        "\n",
        "val flulabel = udf(calcPercentLabel)\n",
        "\n",
        "val trngDf3 = weekOverWeek.withColumn(\"class\", flulabel(weekOverWeek(\"weekOverWeek\")))\n",
        "\n",
        "println(\"Training DF 3\")\n",
        "trngDf3.show\n",
        "\n",
        "val assembler = new VectorAssembler()\n",
        "  .setInputCols(Array(\"locRank\", \"weekOverWeek\",\"week\"))\n",
        "  .setOutputCol(\"features\")\n",
        "\n",
        "val trngDf4 = assembler.transform(trngDf3)\n",
        "\n",
        "val labelIndexer = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")\n",
        "val trngDf5 = labelIndexer.fit(trngDf4).transform(trngDf4)\n",
        "\n",
        "val splitSeed = 5043\n",
        "val Array(trainingData, validationData) = trngDf5.randomSplit(Array(0.7, 0.3), splitSeed)\n",
        "\n",
        "println(\"training data ....\")\n",
        "\n",
        "trainingData.show(100)\n",
        "\n",
        "//////////////////////////////// ///////////////////////////////////// ////////////////////////////////////////\n",
        "\n",
        "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.0234).setTol(0.00000001).setElasticNetParam(0.8)\n",
        "\n",
        "val pipeline = new Pipeline()\n",
        "  .setStages(Array(lr))\n",
        "  \n",
        "val model = pipeline.fit(trainingData)  \n",
        "\n",
        "println(\" $$$$$$ trainingData size >>> \"+trainingData.count())\n",
        "\n",
        "//println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")\n",
        "\n",
        "// Now we can optionally save the fitted pipeline to disk\n",
        "model.write.overwrite().save(\"/home/opt/models/lr-model4\")\n",
        "\n",
        "//////\n",
        "val predictions = model.transform(validationData)\n",
        "\n",
        "print(\"$$$$$$ predictions size >>> \"+predictions.count())\n",
        "\n",
        "predictions.show(100)\n",
        "\n",
        "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n",
        "val accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "val lp = predictions.select( \"label\", \"prediction\")\n",
        "val counttotal = predictions.count()\n",
        "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
        "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
        "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
        "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
        "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
        "val ratioCorrect=correct.toDouble/counttotal.toDouble\n",
        "\n",
        "val  predictionAndLabels =predictions.select(\"rawPrediction\", \"label\").rdd.map(x => (x(0).asInstanceOf[DenseVector](1), x(1).asInstanceOf[Double]))\n",
        "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
        "println(\"area under the precision-recall curve: \" + metrics.areaUnderPR)\n",
        "println(\"area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n",
        "\n",
        "\n",
        "\n",
        "/// RESULTS\n",
        "//Training DF 2\n",
        "//+-------+------------------+----+\n",
        "//|locRank|         weeklyAvg|week|\n",
        "//+-------+------------------+----+\n",
        "//|    5.0|               6.5| 8.0|\n",
        "//|    5.0|               5.0|16.0|\n",
        "//|   33.0|              4.75|16.0|\n",
        "//|   11.0|               4.5|16.0|\n",
        "// and so on ....\n",
        "\n",
        "//Training DF 3 \n",
        "//+-------+------------------+----+-----+\n",
        "//|locRank|         weeklyAvg|week|class|\n",
        "//+-------+------------------+----+-----+\n",
        "//|    5.0|               6.5| 8.0|  1.0|\n",
        "//|    5.0|               5.0|16.0|  1.0|\n",
        "//|   33.0|              4.75|16.0|  1.0|\n",
        "//|   11.0|               4.5|16.0|  1.0|\n",
        "//|    5.0| 4.428571428571429| 7.0|  1.0|\n",
        "\n",
        "//Training DF  with feature vectors\n",
        "\n",
        "//+-------+------------------+----+-----+--------------------+-----+\n",
        "//|locRank|         weeklyAvg|week|class|            features|label|\n",
        "//+-------+------------------+----+-----+--------------------+-----+\n",
        "//|    1.0|               1.0| 9.0|  0.0|       [1.0,1.0,9.0]|  0.0|\n",
        "//|    2.0|               3.0| 7.0|  1.0|       [2.0,3.0,7.0]|  1.0|\n",
        "\n",
        "// Evaluation\n",
        "//evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_b7c8fb21061b\n",
        "//accuracy: Double = 0.989247311827957\n",
        "\n",
        "//counttotal: Long = 133\n",
        "//correct: Long = 128\n",
        "//wrong: Long = 5\n",
        "//truep: Long = 124\n",
        "//falseN: Long = 5\n",
        "//falseP: Long = 0\n",
        "//ratioWrong: Double = 0.03759398496240601\n",
        "//ratioCorrect: Double = 0.9624060150375939\n",
        "\n",
        "//area under the precision-recall curve: 0.8858736171236172\n",
        "//area under the receiver operating characteristic (ROC) curve : 0.989247311827957"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}